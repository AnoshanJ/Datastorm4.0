{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1764,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1765,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Matplotlib defaults\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=14,\n",
    "    titlepad=10,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1766,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../Datasets/\"\n",
    "Train = pd.read_csv(path + '/Historical-transaction-data.csv')\n",
    "StoreInfo = pd.read_csv(path + '/Store-info.csv')\n",
    "Submission_testing = pd.read_csv(path + '/Testing-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1767,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = Train.merge(StoreInfo, on='shop_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1768,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission_testing.drop(\"shop_profile\", axis=1, inplace=True)\n",
    "# Submission_testing = Submission_testing.merge(StoreInfo, on='shop_id', how='left')\n",
    "# Submission_testing.drop(\"shop_profile\", axis=1, inplace=True)\n",
    "Submission_testing = Submission_testing.merge(Train, on='shop_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1769,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission_testing.drop(\"shop_profile\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1770,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1771,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train.to_csv('CombinedData.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1772,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission_testing.to_csv('SubmissionData.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1773,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_cols = ['transaction_date', 'invoice_id', 'customer_id']\n",
    "\n",
    "Train.drop(redundant_cols, axis=1, inplace=True)\n",
    "Submission_testing.drop(redundant_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1774,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_nom = ['item_description',\"shop_id\", \"shop_profile\"]\n",
    "\n",
    "for feature in features_nom:\n",
    "    Train[feature] = Train[feature].astype(\"category\")\n",
    "    if feature == \"shop_profile\":\n",
    "        continue\n",
    "    Submission_testing[feature] = Submission_testing[feature].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1775,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train[\"shop_id\"] = Train[\"shop_id\"].str.replace(\"SHOP\", \"\").astype(int).astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1776,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission_testing[\"shop_id\"] = Submission_testing[\"shop_id\"].str.replace(\"SHOP\", \"\").astype(int).astype(\"category\")\n",
    "StoreInfo[\"shop_id\"] = StoreInfo[\"shop_id\"].str.replace(\"SHOP\", \"\").astype(int).astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1777,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows with null values for item_description or shop_profile\n",
    "Train = Train.dropna(subset=['shop_profile'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1778,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "#import simpleimputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "Submission_testing['shop_profile'] = 'default_value'\n",
    "ct = ColumnTransformer([(\"SimpleImputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\"), [\"item_description\"])], remainder=\"passthrough\")\n",
    "\n",
    "\n",
    "Train = pd.DataFrame(ct.fit_transform(Train), columns=Train.columns)\n",
    "\n",
    "Submission_testing = pd.DataFrame(ct.transform(Submission_testing), columns=Train.columns)\n",
    "\n",
    "Submission_testing.drop(\"shop_profile\", axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1779,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_description</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_price</th>\n",
       "      <th>quantity_sold</th>\n",
       "      <th>shop_area_sq_ft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FIT O MIXED FRUIT 1L</td>\n",
       "      <td>46</td>\n",
       "      <td>270</td>\n",
       "      <td>1</td>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FIT O ORANGE 1L</td>\n",
       "      <td>46</td>\n",
       "      <td>290</td>\n",
       "      <td>1</td>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LEMONADE 1.5L</td>\n",
       "      <td>46</td>\n",
       "      <td>220</td>\n",
       "      <td>2</td>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FIT O MANGO 200ML</td>\n",
       "      <td>46</td>\n",
       "      <td>180</td>\n",
       "      <td>4</td>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FIT O MIXED FRUIT 200ML</td>\n",
       "      <td>46</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          item_description shop_id item_price quantity_sold shop_area_sq_ft\n",
       "0     FIT O MIXED FRUIT 1L      46        270             1             545\n",
       "1          FIT O ORANGE 1L      46        290             1             545\n",
       "2            LEMONADE 1.5L      46        220             2             545\n",
       "3        FIT O MANGO 200ML      46        180             4             545\n",
       "4  FIT O MIXED FRUIT 200ML      46         60             1             545"
      ]
     },
     "execution_count": 1779,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Submission_testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1780,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create price-related features\n",
    "Train['total_sales']= Train['item_price'] * Train['quantity_sold']\n",
    "Submission_testing['total_sales']= Submission_testing['item_price'] * Submission_testing['quantity_sold']\n",
    "\n",
    "# Create a new feature for the total no of unique item_description per shop_id\n",
    "n_unique = Train.groupby(['shop_id'])['item_description'].nunique().reset_index()\n",
    "# Create a new freature for the total no of rows per shop_id\n",
    "n_count = Train.groupby(['shop_id'])['item_description'].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1781,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_n_unique = Submission_testing.groupby(['shop_id'])['item_description'].nunique().reset_index()\n",
    "# Create a new freature for the total no of rows per shop_id\n",
    "sub_n_count = Submission_testing.groupby(['shop_id'])['item_description'].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1782,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_id</th>\n",
       "      <th>total_sales</th>\n",
       "      <th>unique_items</th>\n",
       "      <th>order_count</th>\n",
       "      <th>sales_per_sq_ft</th>\n",
       "      <th>sales_minus_shop_area</th>\n",
       "      <th>sales_plus_shop_area</th>\n",
       "      <th>sales_times_shop_area</th>\n",
       "      <th>shop_area_sq_ft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>126195</td>\n",
       "      <td>32</td>\n",
       "      <td>244</td>\n",
       "      <td>186.128319</td>\n",
       "      <td>125517</td>\n",
       "      <td>126873</td>\n",
       "      <td>85560210</td>\n",
       "      <td>678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112</td>\n",
       "      <td>1150230</td>\n",
       "      <td>36</td>\n",
       "      <td>2236</td>\n",
       "      <td>1721.901198</td>\n",
       "      <td>1149562</td>\n",
       "      <td>1150898</td>\n",
       "      <td>768353640</td>\n",
       "      <td>668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55</td>\n",
       "      <td>892055</td>\n",
       "      <td>37</td>\n",
       "      <td>2275</td>\n",
       "      <td>1238.965278</td>\n",
       "      <td>891335</td>\n",
       "      <td>892775</td>\n",
       "      <td>642279600</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1830845</td>\n",
       "      <td>36</td>\n",
       "      <td>3422</td>\n",
       "      <td>2260.302469</td>\n",
       "      <td>1830035</td>\n",
       "      <td>1831655</td>\n",
       "      <td>1482984450</td>\n",
       "      <td>810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71</td>\n",
       "      <td>895560</td>\n",
       "      <td>36</td>\n",
       "      <td>2307</td>\n",
       "      <td>1272.102273</td>\n",
       "      <td>894856</td>\n",
       "      <td>896264</td>\n",
       "      <td>630474240</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  shop_id total_sales  unique_items  order_count sales_per_sq_ft  \\\n",
       "0       8      126195            32          244      186.128319   \n",
       "1     112     1150230            36         2236     1721.901198   \n",
       "2      55      892055            37         2275     1238.965278   \n",
       "3       3     1830845            36         3422     2260.302469   \n",
       "4      71      895560            36         2307     1272.102273   \n",
       "\n",
       "  sales_minus_shop_area sales_plus_shop_area sales_times_shop_area  \\\n",
       "0                125517               126873              85560210   \n",
       "1               1149562              1150898             768353640   \n",
       "2                891335               892775             642279600   \n",
       "3               1830035              1831655            1482984450   \n",
       "4                894856               896264             630474240   \n",
       "\n",
       "   shop_area_sq_ft  \n",
       "0              678  \n",
       "1              668  \n",
       "2              720  \n",
       "3              810  \n",
       "4              704  "
      ]
     },
     "execution_count": 1782,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_cum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1783,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Aggregate X_train_scaled by shop_id and add all the total_sales values and prevent empty values\n",
    "\n",
    "Train_cum = Train.groupby(['shop_id'], sort=False).agg({'total_sales':'sum'})\n",
    "Submission_testing_cum = Submission_testing.groupby(['shop_id'], sort='False').agg({'total_sales':'sum'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1784,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_id</th>\n",
       "      <th>total_sales</th>\n",
       "      <th>shop_area_sq_ft</th>\n",
       "      <th>shop_profile</th>\n",
       "      <th>unique_items</th>\n",
       "      <th>order_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>126195</td>\n",
       "      <td>678</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>32</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112</td>\n",
       "      <td>1150230</td>\n",
       "      <td>668</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>36</td>\n",
       "      <td>2236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55</td>\n",
       "      <td>892055</td>\n",
       "      <td>720</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>37</td>\n",
       "      <td>2275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1830845</td>\n",
       "      <td>810</td>\n",
       "      <td>High</td>\n",
       "      <td>36</td>\n",
       "      <td>3422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71</td>\n",
       "      <td>895560</td>\n",
       "      <td>704</td>\n",
       "      <td>Low</td>\n",
       "      <td>36</td>\n",
       "      <td>2307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shop_id total_sales  shop_area_sq_ft shop_profile  unique_items  \\\n",
       "0        8      126195              678     Moderate            32   \n",
       "1      112     1150230              668     Moderate            36   \n",
       "2       55      892055              720     Moderate            37   \n",
       "3        3     1830845              810         High            36   \n",
       "4       71      895560              704          Low            36   \n",
       "\n",
       "   order_count  \n",
       "0          244  \n",
       "1         2236  \n",
       "2         2275  \n",
       "3         3422  \n",
       "4         2307  "
      ]
     },
     "execution_count": 1784,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Train_cum = Train_cum.merge(StoreInfo, on='shop_id', how='left')\n",
    "Train_cum = Train_cum.merge(n_unique, on='shop_id', how='left')\n",
    "Train_cum = Train_cum.merge(n_count, on='shop_id', how='left')\n",
    "# rename item_description_x to item_description_count\n",
    "Train_cum.rename(columns={'item_description_x':'unique_items', 'item_description_y':'order_count'}, inplace=True)\n",
    "Train_cum.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1785,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_cum = Train_cum.dropna(subset=['shop_profile'], axis=0)\n",
    "Train_cum.drop(\"shop_profile\", axis=1, inplace=True)\n",
    "\n",
    "Submission_testing_cum = Submission_testing_cum.merge(StoreInfo, on='shop_id', how='left')\n",
    "Submission_testing_cum = Submission_testing_cum.merge(sub_n_unique, on='shop_id', how='left')\n",
    "Submission_testing_cum = Submission_testing_cum.merge(sub_n_count, on='shop_id', how='left')\n",
    "Submission_testing_cum.rename(columns={'item_description_x':'unique_items', 'item_description_y':'order_count'}, inplace=True)\n",
    "Submission_testing_cum.drop(\"shop_profile\", axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1786,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new feature: total_sales per sq ft of the shop\n",
    "Train_cum['sales_per_sq_ft'] = Train_cum['total_sales'] / Train_cum['shop_area_sq_ft']\n",
    "Submission_testing_cum['sales_per_sq_ft'] = Submission_testing_cum['total_sales'] / Submission_testing_cum['shop_area_sq_ft']\n",
    "\n",
    "# new feature : difference between the total_sales and total_sales_per_sq_ft\n",
    "Train_cum['sales_minus_shop_area'] = Train_cum['total_sales'] - Train_cum['shop_area_sq_ft']\n",
    "Submission_testing_cum['sales_minus_shop_area'] = Submission_testing_cum['total_sales'] - Submission_testing_cum['shop_area_sq_ft']\n",
    "\n",
    "# new feature: addition of the total_sales and total_sales_per_sq_ft\n",
    "Train_cum['sales_plus_shop_area'] = Train_cum['total_sales'] + Train_cum['shop_area_sq_ft']\n",
    "Submission_testing_cum['sales_plus_shop_area'] = Submission_testing_cum['total_sales'] + Submission_testing_cum['shop_area_sq_ft']\n",
    "\n",
    "# new feature : multiplication of the total_sales and total_sales_per_sq_ft\n",
    "Train_cum['sales_times_shop_area'] = Train_cum['total_sales'] * Train_cum['shop_area_sq_ft']\n",
    "Submission_testing_cum['sales_times_shop_area'] = Submission_testing_cum['total_sales'] * Submission_testing_cum['shop_area_sq_ft']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1787,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with zero total_sales\n",
    "\n",
    "Train_cum = Train_cum[Train_cum['total_sales'] != 0]\n",
    "Submission_testing_cum = Submission_testing_cum[Submission_testing_cum['total_sales'] != 0]\n",
    "\n",
    "Train_cum.drop(\"shop_area_sq_ft\", axis=1, inplace=True)\n",
    "Train_cum = Train_cum.merge(StoreInfo, on='shop_id', how='left')\n",
    "Submission_testing_cum.drop(\"shop_area_sq_ft\", axis=1, inplace=True)\n",
    "Submission_testing_cum = Submission_testing_cum.merge(StoreInfo, on='shop_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1788,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import labelencoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1789,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new_train = Train_cum[\"shop_profile\"]\n",
    "Train_cum.drop(\"shop_profile\", axis=1, inplace=True)\n",
    "Submission_testing_cum.drop(\"shop_profile\", axis=1, inplace=True)\n",
    "y_new_train = le.fit_transform(y_new_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1790,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_id</th>\n",
       "      <th>total_sales</th>\n",
       "      <th>unique_items</th>\n",
       "      <th>order_count</th>\n",
       "      <th>sales_per_sq_ft</th>\n",
       "      <th>sales_minus_shop_area</th>\n",
       "      <th>sales_plus_shop_area</th>\n",
       "      <th>sales_times_shop_area</th>\n",
       "      <th>shop_area_sq_ft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3084455</td>\n",
       "      <td>36</td>\n",
       "      <td>4800</td>\n",
       "      <td>4842.158556</td>\n",
       "      <td>3083818</td>\n",
       "      <td>3085092</td>\n",
       "      <td>1964797835</td>\n",
       "      <td>637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>2200580</td>\n",
       "      <td>32</td>\n",
       "      <td>3766</td>\n",
       "      <td>5354.209246</td>\n",
       "      <td>2200169</td>\n",
       "      <td>2200991</td>\n",
       "      <td>904438380</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>2570425</td>\n",
       "      <td>37</td>\n",
       "      <td>4103</td>\n",
       "      <td>4166.004862</td>\n",
       "      <td>2569808</td>\n",
       "      <td>2571042</td>\n",
       "      <td>1585952225</td>\n",
       "      <td>617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>1308795</td>\n",
       "      <td>36</td>\n",
       "      <td>2883</td>\n",
       "      <td>1936.087278</td>\n",
       "      <td>1308119</td>\n",
       "      <td>1309471</td>\n",
       "      <td>884745420</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>1973805</td>\n",
       "      <td>36</td>\n",
       "      <td>3799</td>\n",
       "      <td>3306.20603</td>\n",
       "      <td>1973208</td>\n",
       "      <td>1974402</td>\n",
       "      <td>1178361585</td>\n",
       "      <td>597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  shop_id total_sales  unique_items  order_count sales_per_sq_ft  \\\n",
       "0       2     3084455            36         4800     4842.158556   \n",
       "1      19     2200580            32         3766     5354.209246   \n",
       "2      23     2570425            37         4103     4166.004862   \n",
       "3      24     1308795            36         2883     1936.087278   \n",
       "4      29     1973805            36         3799      3306.20603   \n",
       "\n",
       "  sales_minus_shop_area sales_plus_shop_area sales_times_shop_area  \\\n",
       "0               3083818              3085092            1964797835   \n",
       "1               2200169              2200991             904438380   \n",
       "2               2569808              2571042            1585952225   \n",
       "3               1308119              1309471             884745420   \n",
       "4               1973208              1974402            1178361585   \n",
       "\n",
       "   shop_area_sq_ft  \n",
       "0              637  \n",
       "1              411  \n",
       "2              617  \n",
       "3              676  \n",
       "4              597  "
      ]
     },
     "execution_count": 1790,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Submission_testing_cum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1791,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaled_columns = [\"unique_items\", \"order_count\", \"total_sales\", \"sales_per_sq_ft\", \"sales_minus_shop_area\", \"sales_plus_shop_area\", \"sales_times_shop_area\", \"shop_area_sq_ft\"]\n",
    "ct = ColumnTransformer([(\"MinMaxScaler\", MinMaxScaler(), scaled_columns)], remainder=\"passthrough\")\n",
    "\n",
    "\n",
    "scaled_X_train = pd.DataFrame(ct.fit_transform(Train_cum), columns=scaled_columns+[\"shop_id\"])\n",
    "scaled_X_submission = pd.DataFrame(ct.transform(Submission_testing_cum), columns=scaled_columns+[\"shop_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1792,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_items</th>\n",
       "      <th>order_count</th>\n",
       "      <th>total_sales</th>\n",
       "      <th>sales_per_sq_ft</th>\n",
       "      <th>sales_minus_shop_area</th>\n",
       "      <th>sales_plus_shop_area</th>\n",
       "      <th>sales_times_shop_area</th>\n",
       "      <th>shop_area_sq_ft</th>\n",
       "      <th>shop_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.3366</td>\n",
       "      <td>0.17719</td>\n",
       "      <td>0.144192</td>\n",
       "      <td>0.17719</td>\n",
       "      <td>0.17719</td>\n",
       "      <td>0.17823</td>\n",
       "      <td>0.474968</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.34319</td>\n",
       "      <td>0.132518</td>\n",
       "      <td>0.09885</td>\n",
       "      <td>0.132509</td>\n",
       "      <td>0.132526</td>\n",
       "      <td>0.14532</td>\n",
       "      <td>0.54172</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.537006</td>\n",
       "      <td>0.294958</td>\n",
       "      <td>0.194743</td>\n",
       "      <td>0.294932</td>\n",
       "      <td>0.294984</td>\n",
       "      <td>0.36477</td>\n",
       "      <td>0.657253</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.348597</td>\n",
       "      <td>0.133124</td>\n",
       "      <td>0.101961</td>\n",
       "      <td>0.133118</td>\n",
       "      <td>0.13313</td>\n",
       "      <td>0.142239</td>\n",
       "      <td>0.521181</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_items order_count total_sales sales_per_sq_ft sales_minus_shop_area  \\\n",
       "0          0.0         0.0         0.0             0.0                   0.0   \n",
       "1          0.8      0.3366     0.17719        0.144192               0.17719   \n",
       "2          1.0     0.34319    0.132518         0.09885              0.132509   \n",
       "3          0.8    0.537006    0.294958        0.194743              0.294932   \n",
       "4          0.8    0.348597    0.133124        0.101961              0.133118   \n",
       "\n",
       "  sales_plus_shop_area sales_times_shop_area shop_area_sq_ft shop_id  \n",
       "0                  0.0                   0.0        0.487805       8  \n",
       "1              0.17719               0.17823        0.474968     112  \n",
       "2             0.132526               0.14532         0.54172      55  \n",
       "3             0.294984               0.36477        0.657253       3  \n",
       "4              0.13313              0.142239        0.521181      71  "
      ]
     },
     "execution_count": 1792,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1793,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set scaled_X_train shop_id as categorical and other columns as numerical\n",
    "\n",
    "category_columns = [\"shop_id\"]\n",
    "\n",
    "for col in scaled_X_train.columns:\n",
    "    if col not in category_columns:\n",
    "        scaled_X_train[col] = scaled_X_train[col].astype(\"float\")\n",
    "        scaled_X_submission[col] = scaled_X_submission[col].astype(\"float\")\n",
    "    else:\n",
    "        scaled_X_train[col] = scaled_X_train[col].astype(\"category\")\n",
    "        scaled_X_submission[col] = scaled_X_submission[col].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1794,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting the dataset into the Training set and Test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X_train, y_new_train, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1795,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def score_classification(model, df1_x, df1_y, df2_x, df2_y):\n",
    "  print(model)\n",
    "  df1_x = df1_x.copy()\n",
    "  df2_x = df2_x.copy() \n",
    "  ct2 = ColumnTransformer(transformers=[('encoder', OneHotEncoder(handle_unknown='ignore'), [0])], remainder='passthrough')\n",
    "\n",
    "  # One hot encoding for the item_description column\n",
    "\n",
    "  df1_encoded = ct2.fit_transform(df1_x[[\"shop_id\"]])\n",
    "  df1_x = pd.concat([df1_x.drop(\"shop_id\", axis=1).reset_index(drop=True), pd.DataFrame(df1_encoded.toarray())], axis='columns')\n",
    "\n",
    "  df2_encoded = ct2.transform(df2_x[[\"shop_id\"]])\n",
    "  df2_x = pd.concat([df2_x.drop(\"shop_id\", axis=1).reset_index(drop=True), pd.DataFrame(df2_encoded.toarray())], axis='columns')\n",
    "\n",
    "  df1_x.columns = df1_x.columns.astype(str)\n",
    "  df2_x.columns = df2_x.columns.astype(str)\n",
    "  # Fit the model  \n",
    "  model.fit(df1_x, df1_y)\n",
    "\n",
    "  # Make predictions\n",
    "  y_pred = model.predict(df2_x)\n",
    "\n",
    "  # Evaluate the model\n",
    "  accuracy = accuracy_score(df2_y, y_pred)\n",
    "  precision = precision_score(df2_y, y_pred,  average='macro')\n",
    "  recall = recall_score(df2_y, y_pred,  average='macro')\n",
    "  f1 = f1_score(df2_y, y_pred,  average='macro')\n",
    "\n",
    "  print(f\"Accuracy: {accuracy}\")\n",
    "  print(f\"Precision: {precision}\")\n",
    "  print(f\"Recall: {recall}\")\n",
    "  print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1705,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputResult(model, df1_x, df1_y, test, le):\n",
    "    print(model)\n",
    "    df1_x = df1_x.copy()\n",
    "    df2_x = test.copy()\n",
    "   \n",
    "    df1_x.columns = df1_x.columns.astype(str)\n",
    "    df2_x.columns = df2_x.columns.astype(str)\n",
    "    # Fit the model on the training data\n",
    "    model.fit(df1_x, df1_y)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_pred = model.predict(df2_x)\n",
    "    \n",
    "    # Inverse transform the encoded predictions to the original shop profiles\n",
    "    y_pred = le.inverse_transform(y_pred)    \n",
    "    \n",
    "    # Create a dataframe with the predicted shop profiles\n",
    "    results_df = pd.DataFrame({'shop_id': test.shop_id, 'shop_profile': y_pred})    \n",
    "    # rename the shop_id column with the word \"SHOP\" and add a 0 in front of the shop_id\n",
    "    results_df[\"shop_id\"] = results_df[\"shop_id\"].astype(int).astype(str)\n",
    "    results_df[\"shop_id\"] = \"SHOP\" + results_df[\"shop_id\"].str.zfill(3)\n",
    "    # results_df.rename(columns={'shop_id': 'SHOP0' + results_df.shop_id.astype(str)}, inplace=True)\n",
    "    results_df.drop_duplicates(inplace=True)\n",
    "    # Write the dataframe to a CSV file\n",
    "    results_df.to_csv('predictions'+str(model).strip(\"()\")+'.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1706,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 80 entries, 2 to 37\n",
      "Data columns (total 9 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   unique_items           80 non-null     object\n",
      " 1   order_count            80 non-null     object\n",
      " 2   total_sales            80 non-null     object\n",
      " 3   sales_per_sq_ft        80 non-null     object\n",
      " 4   sales_minus_shop_area  80 non-null     object\n",
      " 5   sales_plus_shop_area   80 non-null     object\n",
      " 6   sales_times_shop_area  80 non-null     object\n",
      " 7   shop_area_sq_ft        80 non-null     object\n",
      " 8   shop_id                80 non-null     object\n",
      "dtypes: object(9)\n",
      "memory usage: 6.2+ KB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1743,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1796,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n",
      "Accuracy: 0.25\n",
      "Precision: 0.17845117845117844\n",
      "Recall: 0.3333333333333333\n",
      "F1 Score: 0.2253968253968254\n"
     ]
    }
   ],
   "source": [
    "score_classification(logr, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1745,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1746,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier()\n",
      "Accuracy: 0.4\n",
      "Precision: 0.4047619047619047\n",
      "Recall: 0.398989898989899\n",
      "F1 Score: 0.3814814814814815\n"
     ]
    }
   ],
   "source": [
    "\n",
    "score_classification(dtc, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1747,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1748,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier()\n",
      "Accuracy: 0.3\n",
      "Precision: 0.3703703703703704\n",
      "Recall: 0.2828282828282828\n",
      "F1 Score: 0.273015873015873\n"
     ]
    }
   ],
   "source": [
    "score_classification(rfc, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1749,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1750,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier()\n",
      "Accuracy: 0.35\n",
      "Precision: 0.38888888888888884\n",
      "Recall: 0.3383838383838384\n",
      "F1 Score: 0.3035714285714286\n"
     ]
    }
   ],
   "source": [
    "score_classification(knn, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1797,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1797], line 25\u001b[0m\n\u001b[0;32m     21\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(pipeline, param_grid, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[39m# Fit the grid search object on the data\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     27\u001b[0m \u001b[39m# Print the best parameters found\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest parameters:\u001b[39m\u001b[39m\"\u001b[39m, grid_search\u001b[39m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    823\u001b[0m         clone(base_estimator),\n\u001b[0;32m    824\u001b[0m         X,\n\u001b[0;32m    825\u001b[0m         y,\n\u001b[0;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    835\u001b[0m     )\n\u001b[0;32m    836\u001b[0m )\n\u001b[0;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1061\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1061\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[0;32m   1062\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:938\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    936\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    937\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 938\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[0;32m    939\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    940\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 542\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\concurrent\\futures\\_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[1;32m--> 453\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[0;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "param_grid = {\n",
    "    'knn__n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "    'knn__p': [1, 2],\n",
    "    'knn__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'knn__leaf_size': [10, 20, 30, 40, 50],\n",
    "    'knn__metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski']\n",
    "}\n",
    "\n",
    "# Create a pipeline to preprocess the data and apply KNN\n",
    "pipeline = Pipeline([ \n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Create a grid search object to find the best parameters\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "\n",
    "# Fit the grid search object on the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1668,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5   0.375 0.375 0.375 0.25  0.375 0.25  0.125 0.5   0.375]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=False).split(range(25))\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "knn2 = KNeighborsClassifier(n_neighbors=5)\n",
    "scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1752,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(leaf_size=10, metric='euclidean', p=1, weights='distance')\n",
      "Accuracy: 0.4\n",
      "Precision: 0.4490740740740741\n",
      "Recall: 0.44949494949494945\n",
      "F1 Score: 0.37806637806637816\n"
     ]
    }
   ],
   "source": [
    "# Set the best parameters found by GridSearchCV\n",
    "knn.set_params(**{'algorithm': 'auto', 'leaf_size': 10, 'metric': 'euclidean', 'n_neighbors': 5, 'p': 1, 'weights': 'distance'})\n",
    "\n",
    "score_classification(knn, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1753,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1754,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB()\n",
      "Accuracy: 0.7\n",
      "Precision: 0.4468864468864469\n",
      "Recall: 0.5252525252525252\n",
      "F1 Score: 0.4829059829059828\n"
     ]
    }
   ],
   "source": [
    "\n",
    "score_classification(nb, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1755,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100, max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1674,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 80 entries, 2 to 37\n",
      "Data columns (total 9 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   unique_items           80 non-null     object\n",
      " 1   order_count            80 non-null     object\n",
      " 2   total_sales            80 non-null     object\n",
      " 3   sales_per_sq_ft        80 non-null     object\n",
      " 4   sales_minus_shop_area  80 non-null     object\n",
      " 5   sales_plus_shop_area   80 non-null     object\n",
      " 6   sales_times_shop_area  80 non-null     object\n",
      " 7   shop_area_sq_ft        80 non-null     object\n",
      " 8   shop_id                80 non-null     object\n",
      "dtypes: object(9)\n",
      "memory usage: 6.2+ KB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1756,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, ...)\n",
      "Accuracy: 0.3\n",
      "Precision: 0.34656084656084657\n",
      "Recall: 0.3383838383838384\n",
      "F1 Score: 0.2982905982905983\n"
     ]
    }
   ],
   "source": [
    "\n",
    "score_classification(xgb_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1758,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "F1 Score: 0.49 (+/- 0.19)\n",
      "DecisionTreeClassifier\n",
      "F1 Score: 0.40 (+/- 0.20)\n",
      "KNeighborsClassifier\n",
      "F1 Score: 0.31 (+/- 0.20)\n",
      "RandomForestClassifier\n",
      "F1 Score: 0.43 (+/- 0.08)\n",
      "GaussianNB\n",
      "F1 Score: 0.53 (+/- 0.25)\n"
     ]
    }
   ],
   "source": [
    "# Create a list of classifiers to compare\n",
    "classifiers = [logr, dtc, knn, rfc, nb]\n",
    "\n",
    "# Create X and y data\n",
    "\n",
    "# Cross-validation\n",
    "for clf in classifiers:\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring=lambda clf, X, y: f1_score(y, clf.predict(X), average='macro'))\n",
    "    print(clf.__class__.__name__)\n",
    "    print(\"F1 Score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 80 entries, 2 to 37\n",
      "Data columns (total 7 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   total_sales            80 non-null     float64\n",
      " 1   sales_per_sq_ft        80 non-null     float64\n",
      " 2   sales_minus_shop_area  80 non-null     float64\n",
      " 3   sales_plus_shop_area   80 non-null     float64\n",
      " 4   sales_times_shop_area  80 non-null     float64\n",
      " 5   shop_area_sq_ft        80 non-null     float64\n",
      " 6   shop_id                80 non-null     float64\n",
      "dtypes: float64(7)\n",
      "memory usage: 5.0 KB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1798,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def make_mi_scores(X,y):\n",
    "  X = X.copy()\n",
    "  for colname in X.select_dtypes([\"object\", \"category\"]):\n",
    "    X[colname], _ = X[colname].factorize()\n",
    "  # all discrete features should now have integer dtypes\n",
    "  discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n",
    "  mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n",
    "  mi_scores = pd.Series(mi_scores, name=\"Mutual Information Scores\", index=X.columns)\n",
    "  mi_scores = mi_scores.sort_values(ascending=False)\n",
    "  return mi_scores\n",
    "\n",
    "\n",
    "def plot_mi_scores(scores):\n",
    "  scores = scores.sort_values(ascending=True)\n",
    "  width = np.arange(len(scores))\n",
    "  ticks = list(scores.index)\n",
    "  plt.barh(width,scores)\n",
    "  plt.yticks(width, ticks)\n",
    "  plt.title(\"Mututal Information Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1803,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1803], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mi_scores \u001b[39m=\u001b[39m make_mi_scores(X_train, y_train)\n\u001b[0;32m      2\u001b[0m mi_scores\n",
      "Cell \u001b[1;32mIn[1798], line 9\u001b[0m, in \u001b[0;36mmake_mi_scores\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39m# all discrete features should now have integer dtypes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m discrete_features \u001b[39m=\u001b[39m [pd\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39mtypes\u001b[39m.\u001b[39mis_integer_dtype(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m X\u001b[39m.\u001b[39mdtypes]\n\u001b[1;32m----> 9\u001b[0m mi_scores \u001b[39m=\u001b[39m mutual_info_regression(X, y, discrete_features\u001b[39m=\u001b[39;49mdiscrete_features, random_state\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     10\u001b[0m mi_scores \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(mi_scores, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMutual Information Scores\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mcolumns)\n\u001b[0;32m     11\u001b[0m mi_scores \u001b[39m=\u001b[39m mi_scores\u001b[39m.\u001b[39msort_values(ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py:388\u001b[0m, in \u001b[0;36mmutual_info_regression\u001b[1;34m(X, y, discrete_features, n_neighbors, copy, random_state)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmutual_info_regression\u001b[39m(\n\u001b[0;32m    313\u001b[0m     X, y, \u001b[39m*\u001b[39m, discrete_features\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m, n_neighbors\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, random_state\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[0;32m    314\u001b[0m ):\n\u001b[0;32m    315\u001b[0m     \u001b[39m\"\"\"Estimate mutual information for a continuous target variable.\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \n\u001b[0;32m    317\u001b[0m \u001b[39m    Mutual information (MI) [1]_ between two random variables is a non-negative\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[39m           of a Random Vector\", Probl. Peredachi Inf., 23:2 (1987), 9-16\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 388\u001b[0m     \u001b[39mreturn\u001b[39;00m _estimate_mi(X, y, discrete_features, \u001b[39mFalse\u001b[39;49;00m, n_neighbors, copy, random_state)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py:304\u001b[0m, in \u001b[0;36m_estimate_mi\u001b[1;34m(X, y, discrete_features, discrete_target, n_neighbors, copy, random_state)\u001b[0m\n\u001b[0;32m    297\u001b[0m     y \u001b[39m=\u001b[39m scale(y, with_mean\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    298\u001b[0m     y \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m         \u001b[39m1e-10\u001b[39m\n\u001b[0;32m    300\u001b[0m         \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmaximum(\u001b[39m1\u001b[39m, np\u001b[39m.\u001b[39mmean(np\u001b[39m.\u001b[39mabs(y)))\n\u001b[0;32m    301\u001b[0m         \u001b[39m*\u001b[39m rng\u001b[39m.\u001b[39mstandard_normal(size\u001b[39m=\u001b[39mn_samples)\n\u001b[0;32m    302\u001b[0m     )\n\u001b[1;32m--> 304\u001b[0m mi \u001b[39m=\u001b[39m [\n\u001b[0;32m    305\u001b[0m     _compute_mi(x, y, discrete_feature, discrete_target, n_neighbors)\n\u001b[0;32m    306\u001b[0m     \u001b[39mfor\u001b[39;00m x, discrete_feature \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(_iterate_columns(X), discrete_mask)\n\u001b[0;32m    307\u001b[0m ]\n\u001b[0;32m    309\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(mi)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py:305\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    297\u001b[0m     y \u001b[39m=\u001b[39m scale(y, with_mean\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    298\u001b[0m     y \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m         \u001b[39m1e-10\u001b[39m\n\u001b[0;32m    300\u001b[0m         \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmaximum(\u001b[39m1\u001b[39m, np\u001b[39m.\u001b[39mmean(np\u001b[39m.\u001b[39mabs(y)))\n\u001b[0;32m    301\u001b[0m         \u001b[39m*\u001b[39m rng\u001b[39m.\u001b[39mstandard_normal(size\u001b[39m=\u001b[39mn_samples)\n\u001b[0;32m    302\u001b[0m     )\n\u001b[0;32m    304\u001b[0m mi \u001b[39m=\u001b[39m [\n\u001b[1;32m--> 305\u001b[0m     _compute_mi(x, y, discrete_feature, discrete_target, n_neighbors)\n\u001b[0;32m    306\u001b[0m     \u001b[39mfor\u001b[39;00m x, discrete_feature \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(_iterate_columns(X), discrete_mask)\n\u001b[0;32m    307\u001b[0m ]\n\u001b[0;32m    309\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(mi)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py:161\u001b[0m, in \u001b[0;36m_compute_mi\u001b[1;34m(x, y, x_discrete, y_discrete, n_neighbors)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[39mreturn\u001b[39;00m mutual_info_score(x, y)\n\u001b[0;32m    160\u001b[0m \u001b[39melif\u001b[39;00m x_discrete \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m y_discrete:\n\u001b[1;32m--> 161\u001b[0m     \u001b[39mreturn\u001b[39;00m _compute_mi_cd(y, x, n_neighbors)\n\u001b[0;32m    162\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m x_discrete \u001b[39mand\u001b[39;00m y_discrete:\n\u001b[0;32m    163\u001b[0m     \u001b[39mreturn\u001b[39;00m _compute_mi_cd(x, y, n_neighbors)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py:138\u001b[0m, in \u001b[0;36m_compute_mi_cd\u001b[1;34m(c, d, n_neighbors)\u001b[0m\n\u001b[0;32m    135\u001b[0m c \u001b[39m=\u001b[39m c[mask]\n\u001b[0;32m    136\u001b[0m radius \u001b[39m=\u001b[39m radius[mask]\n\u001b[1;32m--> 138\u001b[0m kd \u001b[39m=\u001b[39m KDTree(c)\n\u001b[0;32m    139\u001b[0m m_all \u001b[39m=\u001b[39m kd\u001b[39m.\u001b[39mquery_radius(c, radius, count_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_distance\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    140\u001b[0m m_all \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(m_all)\n",
      "File \u001b[1;32msklearn\\neighbors\\_binary_tree.pxi:833\u001b[0m, in \u001b[0;36msklearn.neighbors._kd_tree.BinaryTree.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:931\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    929\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n\u001b[0;32m    930\u001b[0m     \u001b[39mif\u001b[39;00m n_samples \u001b[39m<\u001b[39m ensure_min_samples:\n\u001b[1;32m--> 931\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    932\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m sample(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    933\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    934\u001b[0m             \u001b[39m%\u001b[39m (n_samples, array\u001b[39m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m    935\u001b[0m         )\n\u001b[0;32m    937\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_features \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    938\u001b[0m     n_features \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "mi_scores = make_mi_scores(X_train, y_train)\n",
    "mi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def apply_pca(X):\n",
    "  pca= PCA()\n",
    "  df1_x = X.copy()\n",
    "\n",
    "  X_pca = pca.fit_transform(df1_x)\n",
    "  component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "  X_pca = pd.DataFrame(X_pca, columns=component_names)\n",
    "  #create loadings\n",
    "  loadings = pd.DataFrame(\n",
    "  pca.components_.T,\n",
    "  columns = component_names,\n",
    "  index=df1_x.columns,\n",
    "  )\n",
    "  return pca, X_pca, loadings\n",
    "\n",
    "def plot_variance(pca, width=8, dpi=100):\n",
    "  fig, axs = plt.subplots(1,2)\n",
    "  n = pca.n_components_\n",
    "  grid = np.arange(1, n+1)\n",
    "  evr = pca.explained_variance_ratio_\n",
    "  axs[0].bar(grid,evr)\n",
    "  axs[0].set(\n",
    "      xlabel=\"Component\",title=\"% Explained Variance\", ylim=(0.0,1.0)             )\n",
    "  #Cumulative Variance\n",
    "  cv = np.cumsum(evr)\n",
    "  axs[1].plot(np.r_[0,grid], np.r_[0,cv], \"o-\")\n",
    "  axs[1].set(xlabel=\"Component\", title=\"% Cumulatve Variance\", ylim=(0.0, 1.0))\n",
    "  fig.set(figwidth=8, dpi=100)\n",
    "  return axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1802,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2      55\n",
       "73      9\n",
       "97    126\n",
       "62    118\n",
       "19     75\n",
       "     ... \n",
       "75     45\n",
       "9      12\n",
       "72     10\n",
       "12    115\n",
       "37     49\n",
       "Name: shop_id, Length: 80, dtype: category\n",
       "Categories (80, int64): [3, 5, 6, 7, ..., 124, 125, 126, 127]"
      ]
     },
     "execution_count": 1802,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[\"shop_id\"].astype(\"int\").astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1082,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            PC1       PC2       PC3       PC4       PC5  \\\n",
      "total_sales            0.000341  0.460295 -0.021733 -0.098130 -0.333706   \n",
      "sales_per_sq_ft        0.000078  0.419300  0.418745  0.732183  0.335779   \n",
      "sales_minus_shop_area  0.000341  0.460292 -0.021621 -0.098202 -0.333685   \n",
      "sales_plus_shop_area   0.000341  0.460298 -0.021846 -0.098058 -0.333726   \n",
      "sales_times_shop_area  0.000543  0.433988 -0.364258 -0.376370  0.733018   \n",
      "shop_area_sq_ft        0.000532 -0.015061 -0.830994  0.541631 -0.125927   \n",
      "shop_id                1.000000 -0.000731  0.000630 -0.000041 -0.000016   \n",
      "\n",
      "                                PC6           PC7  \n",
      "total_sales           -4.155963e-01 -7.028132e-01  \n",
      "sales_per_sq_ft       -1.163342e-15 -3.986797e-15  \n",
      "sales_minus_shop_area  8.164564e-01 -8.517810e-03  \n",
      "sales_plus_shop_area  -4.008473e-01  7.113234e-01  \n",
      "sales_times_shop_area  4.006035e-16  3.514355e-16  \n",
      "shop_area_sq_ft        1.640815e-04 -9.702942e-05  \n",
      "shop_id                1.695615e-17  1.029889e-17  \n"
     ]
    }
   ],
   "source": [
    "pca, X_pca, loadings = apply_pca(X_train)\n",
    "print(loadings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1804,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(columns=['sales_plus_shop_area'], axis=1, inplace=True)\n",
    "X_test.drop(columns=['sales_plus_shop_area'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1805,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(columns=['sales_minus_shop_area'], axis=1, inplace=True)\n",
    "X_test.drop(columns=['sales_minus_shop_area'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1806,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "F1 Score: 0.47 (+/- 0.17)\n",
      "DecisionTreeClassifier\n",
      "F1 Score: 0.38 (+/- 0.25)\n",
      "KNeighborsClassifier\n",
      "F1 Score: 0.30 (+/- 0.22)\n",
      "RandomForestClassifier\n",
      "F1 Score: 0.40 (+/- 0.07)\n",
      "GaussianNB\n",
      "F1 Score: 0.52 (+/- 0.29)\n"
     ]
    }
   ],
   "source": [
    "for clf in classifiers:\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring=lambda clf, X, y: f1_score(y, clf.predict(X), average='macro'))\n",
    "    print(clf.__class__.__name__)\n",
    "    print(\"F1 Score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1808,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier()\n",
      "Accuracy: 0.45\n",
      "Precision: 0.4861111111111111\n",
      "Recall: 0.45454545454545453\n",
      "F1 Score: 0.43969102792632203\n"
     ]
    }
   ],
   "source": [
    "score_classification(dtc, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1809,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "F1 Score: 0.44 (+/- 0.24)\n",
      "DecisionTreeClassifier\n",
      "F1 Score: 0.40 (+/- 0.21)\n",
      "KNeighborsClassifier\n",
      "F1 Score: 0.30 (+/- 0.22)\n",
      "RandomForestClassifier\n",
      "F1 Score: 0.46 (+/- 0.20)\n",
      "GaussianNB\n",
      "F1 Score: 0.49 (+/- 0.23)\n"
     ]
    }
   ],
   "source": [
    "X_train.drop(columns=['shop_area_sq_ft'], axis=1, inplace=True)\n",
    "X_test.drop(columns=['shop_area_sq_ft'], axis=1, inplace=True)\n",
    "for clf in classifiers:\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring=lambda clf, X, y: f1_score(y, clf.predict(X), average='macro'))\n",
    "    print(clf.__class__.__name__)\n",
    "    print(\"F1 Score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1810,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(leaf_size=10, metric='euclidean', p=1, weights='distance')\n",
      "Accuracy: 0.4\n",
      "Precision: 0.4490740740740741\n",
      "Recall: 0.44949494949494945\n",
      "F1 Score: 0.37806637806637816\n"
     ]
    }
   ],
   "source": [
    "score_classification(knn, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1811,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "F1 Score: 0.44 (+/- 0.14)\n",
      "DecisionTreeClassifier\n",
      "F1 Score: 0.40 (+/- 0.24)\n",
      "KNeighborsClassifier\n",
      "F1 Score: 0.31 (+/- 0.21)\n",
      "RandomForestClassifier\n",
      "F1 Score: 0.43 (+/- 0.24)\n",
      "GaussianNB\n",
      "F1 Score: 0.52 (+/- 0.29)\n"
     ]
    }
   ],
   "source": [
    "X_train.drop(columns=['order_count', 'unique_items'], axis=1, inplace=True)\n",
    "X_test.drop(columns=['order_count', 'unique_items'], axis=1, inplace=True)\n",
    "for clf in classifiers:\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring=lambda clf, X, y: f1_score(y, clf.predict(X), average='macro'))\n",
    "    print(clf.__class__.__name__)\n",
    "    print(\"F1 Score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1812,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(leaf_size=10, metric='euclidean', p=1, weights='distance')\n",
      "Accuracy: 0.65\n",
      "Precision: 0.6222222222222222\n",
      "Recall: 0.6565656565656566\n",
      "F1 Score: 0.6349206349206349\n"
     ]
    }
   ],
   "source": [
    "score_classification(knn, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n"
     ]
    }
   ],
   "source": [
    "outputResult(logr, X_train, y_train, scaled_X_submission, le)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
