{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1098,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1099,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Matplotlib defaults\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=14,\n",
    "    titlepad=10,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1100,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../Datasets/\"\n",
    "Train = pd.read_csv(path + '/Historical-transaction-data.csv')\n",
    "StoreInfo = pd.read_csv(path + '/Store-info.csv')\n",
    "Submission_testing = pd.read_csv(path + '/Testing-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1101,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = Train.merge(StoreInfo, on='shop_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1102,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission_testing.drop(\"shop_profile\", axis=1, inplace=True)\n",
    "# Submission_testing = Submission_testing.merge(StoreInfo, on='shop_id', how='left')\n",
    "# Submission_testing.drop(\"shop_profile\", axis=1, inplace=True)\n",
    "Submission_testing = Submission_testing.merge(Train, on='shop_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1103,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission_testing.drop(\"shop_profile\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1104,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1106,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.to_csv('CombinedData.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_cols = ['item_description', 'transaction_date', 'invoice_id', 'customer_id']\n",
    "\n",
    "Train.drop(redundant_cols, axis=1, inplace=True)\n",
    "Submission_testing.drop(redundant_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_nom = [\"shop_id\", \"shop_profile]\n",
    "\n",
    "for feature in features_nom:\n",
    "    Train[feature] = Train[feature].astype(\"category\")\n",
    "    if feature == \"shop_profile\":\n",
    "        continue\n",
    "    Submission_testing[feature] = Submission_testing[feature].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train[\"shop_id\"] = Train[\"shop_id\"].str.replace(\"SHOP\", \"\").astype(int).astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission_testing[\"shop_id\"] = Submission_testing[\"shop_id\"].str.replace(\"SHOP\", \"\").astype(int).astype(\"category\")\n",
    "StoreInfo[\"shop_id\"] = StoreInfo[\"shop_id\"].str.replace(\"SHOP\", \"\").astype(int).astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows with null values for item_description or shop_profile\n",
    "Train = Train.dropna(subset=['shop_profile'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create price-related features\n",
    "Train['total_sales']= Train['item_price'] * Train['quantity_sold']\n",
    "Submission_testing['total_sales']= Submission_testing['item_price'] * Submission_testing['quantity_sold']\n",
    "\n",
    "# Aggregate X_train_scaled by shop_id and add all the total_sales values and prevent empty values\n",
    "\n",
    "Train_cum = Train.groupby(['shop_id'], sort=False).agg({'total_sales':'sum'})\n",
    "Submission_testing_cum = Submission_testing.groupby(['shop_id'], sort='False').agg({'total_sales':'sum'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Train_cum = Train_cum.merge(StoreInfo, on='shop_id', how='left')\n",
    "Train_cum = Train_cum.dropna(subset=['shop_profile'], axis=0)\n",
    "Train_cum.drop(\"shop_profile\", axis=1, inplace=True)\n",
    "\n",
    "Submission_testing_cum = Submission_testing_cum.merge(StoreInfo, on='shop_id', how='left')\n",
    "Submission_testing_cum.drop(\"shop_profile\", axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new feature: total_sales per sq ft of the shop\n",
    "Train_cum['sales_per_sq_ft'] = Train_cum['total_sales'] / Train_cum['shop_area_sq_ft']\n",
    "Submission_testing_cum['sales_per_sq_ft'] = Submission_testing_cum['total_sales'] / Submission_testing_cum['shop_area_sq_ft']\n",
    "\n",
    "# new feature : difference between the total_sales and total_sales_per_sq_ft\n",
    "Train_cum['sales_minus_shop_area'] = Train_cum['total_sales'] - Train_cum['shop_area_sq_ft']\n",
    "Submission_testing_cum['sales_minus_shop_area'] = Submission_testing_cum['total_sales'] - Submission_testing_cum['shop_area_sq_ft']\n",
    "\n",
    "# new feature: addition of the total_sales and total_sales_per_sq_ft\n",
    "Train_cum['sales_plus_shop_area'] = Train_cum['total_sales'] + Train_cum['shop_area_sq_ft']\n",
    "Submission_testing_cum['sales_plus_shop_area'] = Submission_testing_cum['total_sales'] + Submission_testing_cum['shop_area_sq_ft']\n",
    "\n",
    "# new feature : multiplication of the total_sales and total_sales_per_sq_ft\n",
    "Train_cum['sales_times_shop_area'] = Train_cum['total_sales'] * Train_cum['shop_area_sq_ft']\n",
    "Submission_testing_cum['sales_times_shop_area'] = Submission_testing_cum['total_sales'] * Submission_testing_cum['shop_area_sq_ft']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with zero total_sales\n",
    "\n",
    "Train_cum = Train_cum[Train_cum['total_sales'] != 0]\n",
    "Submission_testing_cum = Submission_testing_cum[Submission_testing_cum['total_sales'] != 0]\n",
    "\n",
    "Train_cum.drop(\"shop_area_sq_ft\", axis=1, inplace=True)\n",
    "Train_cum = Train_cum.merge(StoreInfo, on='shop_id', how='left')\n",
    "Submission_testing_cum.drop(\"shop_area_sq_ft\", axis=1, inplace=True)\n",
    "Submission_testing_cum = Submission_testing_cum.merge(StoreInfo, on='shop_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new_train = Train_cum[\"shop_profile\"]\n",
    "Train_cum.drop(\"shop_profile\", axis=1, inplace=True)\n",
    "Submission_testing_cum.drop(\"shop_profile\", axis=1, inplace=True)\n",
    "y_new_train = le.fit_transform(y_new_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_id</th>\n",
       "      <th>total_sales</th>\n",
       "      <th>sales_per_sq_ft</th>\n",
       "      <th>sales_minus_shop_area</th>\n",
       "      <th>sales_plus_shop_area</th>\n",
       "      <th>sales_times_shop_area</th>\n",
       "      <th>shop_area_sq_ft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3084455</td>\n",
       "      <td>4842.158556</td>\n",
       "      <td>3083818</td>\n",
       "      <td>3085092</td>\n",
       "      <td>1964797835</td>\n",
       "      <td>637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>2200580</td>\n",
       "      <td>5354.209246</td>\n",
       "      <td>2200169</td>\n",
       "      <td>2200991</td>\n",
       "      <td>904438380</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>2570425</td>\n",
       "      <td>4166.004862</td>\n",
       "      <td>2569808</td>\n",
       "      <td>2571042</td>\n",
       "      <td>1585952225</td>\n",
       "      <td>617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>1308795</td>\n",
       "      <td>1936.087278</td>\n",
       "      <td>1308119</td>\n",
       "      <td>1309471</td>\n",
       "      <td>884745420</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>1973805</td>\n",
       "      <td>3306.206030</td>\n",
       "      <td>1973208</td>\n",
       "      <td>1974402</td>\n",
       "      <td>1178361585</td>\n",
       "      <td>597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shop_id  total_sales  sales_per_sq_ft  sales_minus_shop_area  \\\n",
       "0        2      3084455      4842.158556                3083818   \n",
       "1       19      2200580      5354.209246                2200169   \n",
       "2       23      2570425      4166.004862                2569808   \n",
       "3       24      1308795      1936.087278                1308119   \n",
       "4       29      1973805      3306.206030                1973208   \n",
       "\n",
       "   sales_plus_shop_area  sales_times_shop_area  shop_area_sq_ft  \n",
       "0               3085092             1964797835              637  \n",
       "1               2200991              904438380              411  \n",
       "2               2571042             1585952225              617  \n",
       "3               1309471              884745420              676  \n",
       "4               1974402             1178361585              597  "
      ]
     },
     "execution_count": 1056,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Submission_testing_cum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaled_columns = [\"total_sales\", \"sales_per_sq_ft\", \"sales_minus_shop_area\", \"sales_plus_shop_area\", \"sales_times_shop_area\", \"shop_area_sq_ft\"]\n",
    "ct = ColumnTransformer([(\"MinMaxScaler\", MinMaxScaler(), scaled_columns)], remainder=\"passthrough\")\n",
    "\n",
    "\n",
    "scaled_X_train = pd.DataFrame(ct.fit_transform(Train_cum), columns=scaled_columns+[\"shop_id\"])\n",
    "scaled_X_submission = pd.DataFrame(ct.transform(Submission_testing_cum), columns=scaled_columns+[\"shop_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting the dataset into the Training set and Test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X_train, y_new_train, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1059,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def score_classification(model, df1_x, df1_y, df2_x, df2_y):\n",
    "  print(model)\n",
    "  df1_x = df1_x.copy()\n",
    "  df2_x = df2_x.copy() \n",
    "  ct2 = ColumnTransformer(transformers=[('encoder', OneHotEncoder(handle_unknown='ignore'), [0])], remainder='passthrough')\n",
    "\n",
    "  # One hot encoding for the item_description column\n",
    "\n",
    "  df1_encoded = ct2.fit_transform(df1_x[[\"shop_id\"]])\n",
    "  df1_x = pd.concat([df1_x.drop(\"shop_id\", axis=1).reset_index(drop=True), pd.DataFrame(df1_encoded.toarray())], axis='columns')\n",
    "\n",
    "  df2_encoded = ct2.transform(df2_x[[\"shop_id\"]])\n",
    "  df2_x = pd.concat([df2_x.drop(\"shop_id\", axis=1).reset_index(drop=True), pd.DataFrame(df2_encoded.toarray())], axis='columns')\n",
    "\n",
    "  df1_x.columns = df1_x.columns.astype(str)\n",
    "  df2_x.columns = df2_x.columns.astype(str)\n",
    "  # Fit the model  \n",
    "  model.fit(df1_x, df1_y)\n",
    "\n",
    "  # Make predictions\n",
    "  y_pred = model.predict(df2_x)\n",
    "\n",
    "  # Evaluate the model\n",
    "  accuracy = accuracy_score(df2_y, y_pred)\n",
    "  precision = precision_score(df2_y, y_pred,  average='macro')\n",
    "  recall = recall_score(df2_y, y_pred,  average='macro')\n",
    "  f1 = f1_score(df2_y, y_pred,  average='macro')\n",
    "\n",
    "  print(f\"Accuracy: {accuracy}\")\n",
    "  print(f\"Precision: {precision}\")\n",
    "  print(f\"Recall: {recall}\")\n",
    "  print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1060,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputResult(model, df1_x, df1_y, test, le):\n",
    "    print(model)\n",
    "    df1_x = df1_x.copy()\n",
    "    df2_x = test.copy()\n",
    "   \n",
    "    df1_x.columns = df1_x.columns.astype(str)\n",
    "    df2_x.columns = df2_x.columns.astype(str)\n",
    "    # Fit the model on the training data\n",
    "    model.fit(df1_x, df1_y)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_pred = model.predict(df2_x)\n",
    "    \n",
    "    # Inverse transform the encoded predictions to the original shop profiles\n",
    "    y_pred = le.inverse_transform(y_pred)    \n",
    "    \n",
    "    # Create a dataframe with the predicted shop profiles\n",
    "    results_df = pd.DataFrame({'shop_id': test.shop_id, 'shop_profile': y_pred})    \n",
    "    # rename the shop_id column with the word \"SHOP\" and add a 0 in front of the shop_id\n",
    "    results_df[\"shop_id\"] = results_df[\"shop_id\"].astype(int).astype(str)\n",
    "    results_df[\"shop_id\"] = \"SHOP\" + results_df[\"shop_id\"].str.zfill(3)\n",
    "    # results_df.rename(columns={'shop_id': 'SHOP0' + results_df.shop_id.astype(str)}, inplace=True)\n",
    "    results_df.drop_duplicates(inplace=True)\n",
    "    # Write the dataframe to a CSV file\n",
    "    results_df.to_csv('predictions'+str(model).strip(\"()\")+'.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1061,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1062,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n",
      "Accuracy: 0.25\n",
      "Precision: 0.2161172161172161\n",
      "Recall: 0.3333333333333333\n",
      "F1 Score: 0.24679487179487178\n"
     ]
    }
   ],
   "source": [
    "score_classification(logr, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1063,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1064,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier()\n",
      "Accuracy: 0.3\n",
      "Precision: 0.3333333333333333\n",
      "Recall: 0.2828282828282828\n",
      "F1 Score: 0.2793650793650794\n"
     ]
    }
   ],
   "source": [
    "\n",
    "score_classification(dtc, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1065,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier()\n",
      "Accuracy: 0.35\n",
      "Precision: 0.4166666666666667\n",
      "Recall: 0.3131313131313131\n",
      "F1 Score: 0.3238095238095238\n"
     ]
    }
   ],
   "source": [
    "score_classification(rfc, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1067,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1068,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier()\n",
      "Accuracy: 0.45\n",
      "Precision: 0.37566137566137564\n",
      "Recall: 0.3737373737373737\n",
      "F1 Score: 0.3717948717948718\n"
     ]
    }
   ],
   "source": [
    "score_classification(knn, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1069,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'knn__algorithm': 'auto', 'knn__leaf_size': 10, 'knn__metric': 'euclidean', 'knn__n_neighbors': 5, 'knn__p': 1, 'knn__weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "param_grid = {\n",
    "    'knn__n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "    'knn__p': [1, 2],\n",
    "    'knn__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'knn__leaf_size': [10, 20, 30, 40, 50],\n",
    "    'knn__metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski']\n",
    "}\n",
    "\n",
    "# Create a pipeline to preprocess the data and apply KNN\n",
    "pipeline = Pipeline([ \n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Create a grid search object to find the best parameters\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "\n",
    "# Fit the grid search object on the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5   0.375 0.375 0.375 0.25  0.375 0.25  0.125 0.5   0.375]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=False).split(range(25))\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "knn2 = KNeighborsClassifier(n_neighbors=5)\n",
    "scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1071,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(leaf_size=10, metric='euclidean', p=1, weights='distance')\n",
      "Accuracy: 0.5\n",
      "Precision: 0.5079365079365079\n",
      "Recall: 0.5656565656565656\n",
      "F1 Score: 0.5037037037037037\n"
     ]
    }
   ],
   "source": [
    "# Set the best parameters found by GridSearchCV\n",
    "knn.set_params(**{'algorithm': 'auto', 'leaf_size': 10, 'metric': 'euclidean', 'n_neighbors': 5, 'p': 1, 'weights': 'distance'})\n",
    "\n",
    "score_classification(knn, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1072,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB()\n",
      "Accuracy: 0.7\n",
      "Precision: 0.4468864468864469\n",
      "Recall: 0.5252525252525252\n",
      "F1 Score: 0.4829059829059828\n"
     ]
    }
   ],
   "source": [
    "\n",
    "score_classification(nb, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100, max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1075,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, ...)\n",
      "Accuracy: 0.4\n",
      "Precision: 0.4305555555555555\n",
      "Recall: 0.398989898989899\n",
      "F1 Score: 0.38413547237076656\n"
     ]
    }
   ],
   "source": [
    "\n",
    "score_classification(xgb_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "F1 Score: 0.45 (+/- 0.16)\n",
      "DecisionTreeClassifier\n",
      "F1 Score: 0.35 (+/- 0.18)\n",
      "KNeighborsClassifier\n",
      "F1 Score: 0.32 (+/- 0.20)\n",
      "RandomForestClassifier\n",
      "F1 Score: 0.44 (+/- 0.08)\n",
      "GaussianNB\n",
      "F1 Score: 0.52 (+/- 0.25)\n",
      "XGBClassifier\n",
      "F1 Score: 0.45 (+/- 0.10)\n"
     ]
    }
   ],
   "source": [
    "# Create a list of classifiers to compare\n",
    "classifiers = [logr, dtc, knn, rfc, nb, xgb_model]\n",
    "\n",
    "# Create X and y data\n",
    "\n",
    "# Cross-validation\n",
    "for clf in classifiers:\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring=lambda clf, X, y: f1_score(y, clf.predict(X), average='macro'))\n",
    "    print(clf.__class__.__name__)\n",
    "    print(\"F1 Score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 80 entries, 2 to 37\n",
      "Data columns (total 7 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   total_sales            80 non-null     float64\n",
      " 1   sales_per_sq_ft        80 non-null     float64\n",
      " 2   sales_minus_shop_area  80 non-null     float64\n",
      " 3   sales_plus_shop_area   80 non-null     float64\n",
      " 4   sales_times_shop_area  80 non-null     float64\n",
      " 5   shop_area_sq_ft        80 non-null     float64\n",
      " 6   shop_id                80 non-null     float64\n",
      "dtypes: float64(7)\n",
      "memory usage: 5.0 KB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def make_mi_scores(X,y):\n",
    "  X = X.copy()\n",
    "  for colname in X.select_dtypes([\"object\", \"category\"]):\n",
    "    X[colname], _ = X[colname].factorize()\n",
    "  # all discrete features should now have integer dtypes\n",
    "  discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n",
    "  mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n",
    "  mi_scores = pd.Series(mi_scores, name=\"Mutual Information Scores\", index=X.columns)\n",
    "  mi_scores = mi_scores.sort_values(ascending=False)\n",
    "  return mi_scores\n",
    "\n",
    "\n",
    "def plot_mi_scores(scores):\n",
    "  scores = scores.sort_values(ascending=True)\n",
    "  width = np.arange(len(scores))\n",
    "  ticks = list(scores.index)\n",
    "  plt.barh(width,scores)\n",
    "  plt.yticks(width, ticks)\n",
    "  plt.title(\"Mututal Information Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1079,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sales_times_shop_area    0.213485\n",
       "sales_per_sq_ft          0.141548\n",
       "shop_area_sq_ft          0.084893\n",
       "sales_minus_shop_area    0.053917\n",
       "total_sales              0.052131\n",
       "sales_plus_shop_area     0.052131\n",
       "shop_id                  0.037835\n",
       "Name: Mutual Information Scores, dtype: float64"
      ]
     },
     "execution_count": 1079,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mi_scores = make_mi_scores(X_train, y_train)\n",
    "mi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def apply_pca(X):\n",
    "  pca= PCA()\n",
    "  df1_x = X.copy()\n",
    "\n",
    "  X_pca = pca.fit_transform(df1_x)\n",
    "  component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "  X_pca = pd.DataFrame(X_pca, columns=component_names)\n",
    "  #create loadings\n",
    "  loadings = pd.DataFrame(\n",
    "  pca.components_.T,\n",
    "  columns = component_names,\n",
    "  index=df1_x.columns,\n",
    "  )\n",
    "  return pca, X_pca, loadings\n",
    "\n",
    "def plot_variance(pca, width=8, dpi=100):\n",
    "  fig, axs = plt.subplots(1,2)\n",
    "  n = pca.n_components_\n",
    "  grid = np.arange(1, n+1)\n",
    "  evr = pca.explained_variance_ratio_\n",
    "  axs[0].bar(grid,evr)\n",
    "  axs[0].set(\n",
    "      xlabel=\"Component\",title=\"% Explained Variance\", ylim=(0.0,1.0)             )\n",
    "  #Cumulative Variance\n",
    "  cv = np.cumsum(evr)\n",
    "  axs[1].plot(np.r_[0,grid], np.r_[0,cv], \"o-\")\n",
    "  axs[1].set(xlabel=\"Component\", title=\"% Cumulatve Variance\", ylim=(0.0, 1.0))\n",
    "  fig.set(figwidth=8, dpi=100)\n",
    "  return axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2      55\n",
       "73      9\n",
       "97    126\n",
       "62    118\n",
       "19     75\n",
       "     ... \n",
       "75     45\n",
       "9      12\n",
       "72     10\n",
       "12    115\n",
       "37     49\n",
       "Name: shop_id, Length: 80, dtype: category\n",
       "Categories (80, int64): [3, 5, 6, 7, ..., 124, 125, 126, 127]"
      ]
     },
     "execution_count": 1081,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[\"shop_id\"].astype(\"int\").astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1082,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            PC1       PC2       PC3       PC4       PC5  \\\n",
      "total_sales            0.000341  0.460295 -0.021733 -0.098130 -0.333706   \n",
      "sales_per_sq_ft        0.000078  0.419300  0.418745  0.732183  0.335779   \n",
      "sales_minus_shop_area  0.000341  0.460292 -0.021621 -0.098202 -0.333685   \n",
      "sales_plus_shop_area   0.000341  0.460298 -0.021846 -0.098058 -0.333726   \n",
      "sales_times_shop_area  0.000543  0.433988 -0.364258 -0.376370  0.733018   \n",
      "shop_area_sq_ft        0.000532 -0.015061 -0.830994  0.541631 -0.125927   \n",
      "shop_id                1.000000 -0.000731  0.000630 -0.000041 -0.000016   \n",
      "\n",
      "                                PC6           PC7  \n",
      "total_sales           -4.155963e-01 -7.028132e-01  \n",
      "sales_per_sq_ft       -1.163342e-15 -3.986797e-15  \n",
      "sales_minus_shop_area  8.164564e-01 -8.517810e-03  \n",
      "sales_plus_shop_area  -4.008473e-01  7.113234e-01  \n",
      "sales_times_shop_area  4.006035e-16  3.514355e-16  \n",
      "shop_area_sq_ft        1.640815e-04 -9.702942e-05  \n",
      "shop_id                1.695615e-17  1.029889e-17  \n"
     ]
    }
   ],
   "source": [
    "pca, X_pca, loadings = apply_pca(X_train)\n",
    "print(loadings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1084,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(columns=['sales_plus_shop_area'], axis=1, inplace=True)\n",
    "X_test.drop(columns=['sales_plus_shop_area'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1086,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(columns=['sales_minus_shop_area'], axis=1, inplace=True)\n",
    "X_test.drop(columns=['sales_minus_shop_area'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1087,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "F1 Score: 0.43 (+/- 0.21)\n",
      "DecisionTreeClassifier\n",
      "F1 Score: 0.37 (+/- 0.21)\n",
      "KNeighborsClassifier\n",
      "F1 Score: 0.31 (+/- 0.21)\n",
      "RandomForestClassifier\n",
      "F1 Score: 0.46 (+/- 0.12)\n",
      "GaussianNB\n",
      "F1 Score: 0.50 (+/- 0.31)\n",
      "XGBClassifier\n",
      "F1 Score: 0.45 (+/- 0.10)\n"
     ]
    }
   ],
   "source": [
    "for clf in classifiers:\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring=lambda clf, X, y: f1_score(y, clf.predict(X), average='macro'))\n",
    "    print(clf.__class__.__name__)\n",
    "    print(\"F1 Score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1092,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(leaf_size=10, metric='euclidean', p=1, weights='distance')\n",
      "Accuracy: 0.5\n",
      "Precision: 0.5079365079365079\n",
      "Recall: 0.5656565656565656\n",
      "F1 Score: 0.5037037037037037\n"
     ]
    }
   ],
   "source": [
    "score_classification(knn, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1094,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "F1 Score: 0.44 (+/- 0.14)\n",
      "DecisionTreeClassifier\n",
      "F1 Score: 0.35 (+/- 0.26)\n",
      "KNeighborsClassifier\n",
      "F1 Score: 0.31 (+/- 0.21)\n",
      "RandomForestClassifier\n",
      "F1 Score: 0.44 (+/- 0.18)\n",
      "GaussianNB\n",
      "F1 Score: 0.52 (+/- 0.29)\n",
      "XGBClassifier\n",
      "F1 Score: 0.45 (+/- 0.21)\n"
     ]
    }
   ],
   "source": [
    "X_train.drop(columns=['shop_area_sq_ft'], axis=1, inplace=True)\n",
    "X_test.drop(columns=['shop_area_sq_ft'], axis=1, inplace=True)\n",
    "for clf in classifiers:\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring=lambda clf, X, y: f1_score(y, clf.predict(X), average='macro'))\n",
    "    print(clf.__class__.__name__)\n",
    "    print(\"F1 Score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1095,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(leaf_size=10, metric='euclidean', p=1, weights='distance')\n",
      "Accuracy: 0.65\n",
      "Precision: 0.6222222222222222\n",
      "Recall: 0.6565656565656566\n",
      "F1 Score: 0.6349206349206349\n"
     ]
    }
   ],
   "source": [
    "score_classification(knn, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1097,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24 entries, 0 to 23\n",
      "Data columns (total 4 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   total_sales            24 non-null     float64\n",
      " 1   sales_per_sq_ft        24 non-null     float64\n",
      " 2   sales_times_shop_area  24 non-null     float64\n",
      " 3   shop_id                24 non-null     float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 896.0 bytes\n"
     ]
    }
   ],
   "source": [
    "scaled_X_submission = scaled_X_submission[X_train.columns]\n",
    "scaled_X_submission.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n"
     ]
    }
   ],
   "source": [
    "outputResult(logr, X_train, y_train, scaled_X_submission, le)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
